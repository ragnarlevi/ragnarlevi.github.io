<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Ragnar</title><link>https://ragnarlevi.github.io/</link><description>Recent content on Ragnar</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Mon, 11 Apr 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://ragnarlevi.github.io/index.xml" rel="self" type="application/rss+xml"/><item><title>Neural Networks for Insurance Pricing</title><link>https://ragnarlevi.github.io/post/nnfun/</link><pubDate>Mon, 11 Apr 2022 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/nnfun/</guid><description>In this notebook we will be looking at a typicall insurance pricing data set and test GLM and NN models. A typical assumption is that the repsone is poisson: $$ P(Y = y) = \frac{\lambda^y \exp(-\lambda)}{y!}$$ To note a shortcoming of the Poisson distribution is that the mean is equal to the variance, and thus one might use quasi-Poisson or a negative binomial instead. Another shortcoming is that insurance claims are usally zero inflated and thus a zero-inflated poisson model or a hurdle poisson model might be used instead.</description></item><item><title>Generalized Linear Models</title><link>https://ragnarlevi.github.io/post/glm/</link><pubDate>Wed, 02 Mar 2022 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/glm/</guid><description>In this notebook, I am going to note down elements of generalized linear models (GLM). GLMs come from the exponential family where a response $y$ has the following distribution $$f(y;\theta) =\exp\Big(\frac{y\theta - b(\theta)}{a(\phi)} + c(y,\theta)\Big)$$ where $\theta$ is the canonical parameter and $\phi$ is the dispersion parameter. Before continuing we define the score and derive the a formula for the expected score and the information.
The score is the derivative of the log-likelihood and we can derive the expected score as, using $f = f(y;\theta)$ :</description></item><item><title>Hidden Markov Models</title><link>https://ragnarlevi.github.io/post/hmm/</link><pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/hmm/</guid><description>In this notebook I will be exploring a Hidden Markov Model (HMM) to classify bull and bear states of a simulated financial instrument.
library(depmixS4) library(ggplot2) library(matrixStats) Simulate the data. Assume returns are normally distributed. Where the mean and the standard deviation depends on the state of the financial market. We will only consider 2 states.
set.seed(42) Nk_lower &amp;lt;- 50 Nk_upper &amp;lt;- 150 bull_mean &amp;lt;- 0.1 bull_var &amp;lt;- 0.1 bear_mean &amp;lt;- -0.</description></item><item><title>Quntitative Risk Analysis</title><link>https://ragnarlevi.github.io/post/extremes/</link><pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/extremes/</guid><description>In this notebook I will discuss the basic ideas of modelling quantiles and copula dependence. These ideas are important for risk managers as International banking regulations require banks to pay specific attention to the probability of large losses over short periods of time and undeerestimation of dependence among extreme risks can lead to serious consequences, as for instance those we experienced during the last financial crisis. Including extreme risks in probabilistic models is recognized nowadays as a necessary condition for good risk management in any institution, and not restricted anymore to reinsurance companies, who are the providers of covers for natural catastrophes.</description></item><item><title>Deep Graph Kernels</title><link>https://ragnarlevi.github.io/post/deepkernel/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/deepkernel/</guid><description>Deep Graph Kernels In this notebook I will be constructing a deep graph kernel based on this paper. It utilizes the Weisfeiler-Lehman isomorphism test algorithm and/or the shortest-path algorithm. The kernel is then used for graph classification. The code is taken from jcatw which again cites Pinar Yanardag as the original author. The code is adjusted to accommodate the networkx library and python 3.
import networkx as nx import pandas as pd import numpy as np import re from nltk.</description></item><item><title>Group Identification in Stock Markets</title><link>https://ragnarlevi.github.io/post/correlationnetwork/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/correlationnetwork/</guid><description>Introduction In this notebook I will be trying to replicate the work of Dong-Hee Kim and Hawoong Jeong. The goal is to look at the eigen decomposition of the return correlation matrix. The decomposition allows us to separate the correlation into three parts. Namely, the marketwide effect, group correlation matrix and a random noise part. By filtering out the marketwide effect and the random noise, we can look at nontrivial correlation of stock groups.</description></item><item><title>About</title><link>https://ragnarlevi.github.io/about/</link><pubDate>Sat, 04 Sep 2021 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/about/</guid><description>Ragnar Leví­ Guðmundarson Heriot-Watt, Centre for Networks and Enterprise Excellence I started my PhD at Heriot-Watt University in January 2021 at the Edinburgh campus. My supervisors are Prof. Dimitris Christopoulos, Prof. Gareth Peters and George Tzougas. Generally, I am most interested in problems involving statistics and machine learning, with applications in risk management and insurance. My current research is on Graphical models and network hypothesis testing frameworks with applications in ESG portfolio optimization.</description></item><item><title>Data Visualization and Data Reduction</title><link>https://ragnarlevi.github.io/post/data_visualization/</link><pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/data_visualization/</guid><description>Introduction In this notebook I will be trying out some dimensionality reduction techniques which can be used for modelling and/or data visualization. Dimensional reduction occupies a central position in many fields. In essence, the goal is to change the representation of data sets, originally in a form involving a large number of variables, into a low-dimensional description. The main difference between data reduction and data visualization is that some data visualization techniques can not be used on unseen data, thus they will not be useful in the modelling part.</description></item></channel></rss>