<html><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><title>Ragnar - Gaussian Process Classification</title><meta content="Classification,Kernel" name=keywords><meta content="Ragnar - In this notebook I will be trying to fit a Gaussian Process classification from scratch on very simple simulated data. I will use the laplacian, expectation propagation, and MH MCMC.
import numpy as np import matplotlib.pyplot as plt from sklearn.metrics.pairwise import rbf_kernel import seaborn as sns from scipy.stats import norm from scipy.optimize import minimize import tqdm Create dataset
x = np.array(list(range(100))) y = np.ones(100) y[(x> 25) & (x<60)] = 0 #plt." name=description><meta name=viewport content="width=device-width,initial-scale=1"><link rel=stylesheet href=/layui/css/layui.css><link rel=stylesheet href=/self/css/default.css><script src=/layui/layui.js></script>
<link rel=stylesheet async href=/self/css/markdown.min.css><link rel=stylesheet async href=/self/css/gallery.css><script src=https://cdn.jsdelivr.net/npm/jquery@3.5.1/dist/jquery.min.js integrity="sha256-9/aliU8dGd2tb6OSsuzixeV4y/faTqgFtohetphbbj0=" crossorigin=anonymous></script>
<link rel=stylesheet href=https://cdn.jsdelivr.net/npm/font-awesome@4.7.0/css/font-awesome.min.css integrity="sha256-eZrrJcwDc/3uDhsdt61sL2oOBY362qM3lon1gyExkL0=" crossorigin=anonymous><script async src=https://cdn.jsdelivr.net/npm/lazysizes@5.2.0/lazysizes.min.js integrity="sha256-h2tMEmhemR2IN4wbbdNjj9LaDIjzwk2hralQwfJmBOE=" crossorigin=anonymous></script></head><body><header><script type=text/x-mathjax-config>
      MathJax.Hub.Config({"HTML-CSS": { preferredFont: "TeX", availableFonts: ["STIX","TeX"], linebreaks: { automatic:true }, EqnChunk: (MathJax.Hub.Browser.isMobile ? 10 : 50) },
          tex2jax: { inlineMath: [ ["$", "$"], ["\\(", "\\)"] ], displayMath: [ ["$$","$$"], ["\\[", "\\]"] ], processEscapes: true, ignoreClass: "tex2jax_ignore|dno" },
          TeX: {  noUndefined: { attributes: { mathcolor: "red", mathbackground: "#FFEEEE", mathsize: "90%" } } },
          messageStyle: "none"
      });
  </script><script type=text/javascript src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS_HTML"></script></header><header class="layui-header layui-bg-cyan"><a class=nav-self-logo href=/>Ragnar</a><ul class="layui-nav layui-layout-right layui-bg-cyan" lay-filter><li class=layui-nav-item id=nav_big><a href=/publications/>Publications</a></li><li class=layui-nav-item id=nav_big><a href=/post/>Posts</a></li><li class=layui-nav-item id=nav_big><a href=/about/>About</a></li><li class=layui-nav-item id=nav_small><a href=javascript:;><i class="layui-icon layui-icon-app" style=font-size:24px></i></a><dl class=layui-nav-child><dd><a href=/Publications/>Publications</a></dd><dd><a href=/post/>Posts</a></dd><dd><a href=/about/>About</a></dd></dl></li></ul></header><script>layui.use("element",function(){var e=layui.element})</script><div id=content style=min-height:80%><div class=layui-container style=margin-bottom:10px><div class="layui-row layui-col-space10"><div class="layui-col-md8 layui-col-sm12 layui-col-xs12"><div class="layui-card single-card"><br><blockquote class="self-elem-quote self-elem-quote-bg-red markdown-body single-title"><h1>Gaussian Process Classification</h1><h3 style=margin-top:10px;margin-bottom:10px><i class="layui-icon layui-icon-date" style=font-size:28px;vertical-align:-2px></i>
<span>2022-09-18</span>
<i class="layui-icon layui-icon-tabs" style=font-size:22px;vertical-align:1px;margin-right:2px></i>
<a href=/tags/classification/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Classification</span></a>
<a href=/tags/kernel/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Kernel</span></a></h3></blockquote><div class="layui-card-body markdown-body single-content"><p>In this notebook I will be trying to fit a Gaussian Process classification from scratch on very simple simulated data. I will use the laplacian, expectation propagation, and MH MCMC.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>import</span> numpy <span style=color:#66d9ef>as</span> np
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> matplotlib.pyplot <span style=color:#66d9ef>as</span> plt
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> sklearn.metrics.pairwise <span style=color:#f92672>import</span> rbf_kernel
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> seaborn <span style=color:#66d9ef>as</span> sns
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> scipy.stats <span style=color:#f92672>import</span> norm
</span></span><span style=display:flex><span><span style=color:#f92672>from</span> scipy.optimize <span style=color:#f92672>import</span> minimize
</span></span><span style=display:flex><span><span style=color:#f92672>import</span> tqdm
</span></span></code></pre></div><p>Create dataset</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>x <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>array(list(range(<span style=color:#ae81ff>100</span>)))
</span></span><span style=display:flex><span>y <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>ones(<span style=color:#ae81ff>100</span>)
</span></span><span style=display:flex><span>y[(x<span style=color:#f92672>&gt;</span> <span style=color:#ae81ff>25</span>) <span style=color:#f92672>&amp;</span> (x<span style=color:#f92672>&lt;</span><span style=color:#ae81ff>60</span>)] <span style=color:#f92672>=</span> <span style=color:#ae81ff>0</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#75715e>#plt.scatter(x,y)</span>
</span></span><span style=display:flex><span>train_index <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>choice(list(range(<span style=color:#ae81ff>100</span>)), <span style=color:#ae81ff>20</span>, replace<span style=color:#f92672>=</span><span style=color:#66d9ef>False</span>)
</span></span><span style=display:flex><span>train_index <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sort(train_index)
</span></span><span style=display:flex><span>y_train <span style=color:#f92672>=</span> y[train_index]
</span></span><span style=display:flex><span>y_test<span style=color:#f92672>=</span> y[<span style=color:#f92672>~</span>np<span style=color:#f92672>.</span>isin(list(range(<span style=color:#ae81ff>100</span>)), train_index)]
</span></span><span style=display:flex><span>x_train <span style=color:#f92672>=</span> x[train_index]
</span></span><span style=display:flex><span>x_test <span style=color:#f92672>=</span> x[<span style=color:#f92672>~</span>np<span style=color:#f92672>.</span>isin(list(range(<span style=color:#ae81ff>100</span>)), train_index)]
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x_train,y_train)
</span></span><span style=display:flex><span>print(x_train<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>print(x_test<span style=color:#f92672>.</span>shape)
</span></span></code></pre></div><pre><code>(20,)
(80,)
</code></pre><p><img src=GP_fit_algo_2_1.png alt=png></p><p>The problem with Gaussian Process classification is that the posterior</p><p>$$p(f_{new} | X, y,x_{new}) = \int p(f_{new} | X,x_{new}, f) p(f | X,y ) df$$</p><p>,where $f_{new}$ is the test function and is $x_{new}$ is the test input ,is not tractable . We will look at methods that approximate the posterior to make the integral tractable. We find some distribution $q$ to be close to $p(f | X,y )$. We know of to deal with Guassian so we assume $q$ is Gaussian.</p><p>Once we have found $q$ we can approximate the integral by replacing $p(f|x,y)$ with $q(f|x,y;\mu, \Sigma)$ (once we have found $\mu$ and $\Sigma$). We know that the two Gaussians integrated together will be another Gaussian with mean:</p><p>$$E[f_{new} | X, y,x_{new}] = E[E[f_{new} | X, y,x_{new},f]| X, y,x_{new}] = E[K_{new}K^{-1}f| X, y,x_{new}] = K_{new}K^{-1}\mu$$</p><p>and variance</p><p>$$
\begin{split}
V[f_{new} | X, y,x_{new}] &= E[V[f_{new} | X, y,x_{new},f]| X, y,x_{new}] + V[E[f_{new} | X, y,x_{new},f]| X, y,x_{new}] \\
&= E[K_{newnew} -K_{new}K^{-1}K_{new}^T| X, y,x_{new}] +V[K_{new}K^{-1}f| X, y,x_{new}] \\
&= K_{newnew} -K_{new}K^{-1}K_{new}^T + K_{new}K^{-1}\Sigma K^{-1}K_{new}^T
\end{split}
$$</p><p>where $K_{newnew} = K(X_{new},X_{new})$ and $K_{new} = K(X,X_{new})$</p><p>Functions for the sigmoid, log-likelihood. Also, calculate the kernel assuming an RBF kernel. Just set some gamma parameters that should be good enough</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>gamma <span style=color:#f92672>=</span> <span style=color:#ae81ff>0.005</span>
</span></span><span style=display:flex><span><span style=color:#75715e>#from scipy.special import expit as sigmoid</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>sigma</span>(f):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#ae81ff>1.0</span><span style=color:#f92672>/</span>(<span style=color:#ae81ff>1.0</span><span style=color:#f92672>+</span>np<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>f))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>log_lik</span>(y,f):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>log(<span style=color:#ae81ff>1</span><span style=color:#f92672>+</span>np<span style=color:#f92672>.</span>exp(<span style=color:#f92672>-</span>y<span style=color:#f92672>*</span>f))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>K <span style=color:#f92672>=</span> rbf_kernel(np<span style=color:#f92672>.</span>expand_dims(x_train,<span style=color:#ae81ff>1</span>), gamma <span style=color:#f92672>=</span> gamma)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>heatmap(K)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>Kstar <span style=color:#f92672>=</span> rbf_kernel(np<span style=color:#f92672>.</span>expand_dims(x_train,<span style=color:#ae81ff>1</span>),np<span style=color:#f92672>.</span>expand_dims(x_test,<span style=color:#ae81ff>1</span>), gamma <span style=color:#f92672>=</span> gamma)
</span></span><span style=display:flex><span>Ktest <span style=color:#f92672>=</span> rbf_kernel(np<span style=color:#f92672>.</span>expand_dims(x_test,<span style=color:#ae81ff>1</span>), gamma <span style=color:#f92672>=</span> gamma)
</span></span><span style=display:flex><span>print(Ktest<span style=color:#f92672>.</span>shape)
</span></span><span style=display:flex><span>sns<span style=color:#f92672>.</span>heatmap(K)
</span></span></code></pre></div><p><img src=GP_fit_algo_5_2.png alt=png></p><h1 id=laplace-approximation>Laplace approximation</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>fit_laplace</span>(y,K):
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    f <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(len(y))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> _ <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>100</span>):
</span></span><span style=display:flex><span>        t <span style=color:#f92672>=</span> y
</span></span><span style=display:flex><span>        W <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>np<span style=color:#f92672>.</span>diag(<span style=color:#f92672>-</span>sigma(f)<span style=color:#f92672>*</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>sigma(f)))
</span></span><span style=display:flex><span>        d <span style=color:#f92672>=</span> t <span style=color:#f92672>-</span> sigma(f)
</span></span><span style=display:flex><span>        B <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>identity(W<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(W), K)<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(W))
</span></span><span style=display:flex><span>        L <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>cholesky(B)
</span></span><span style=display:flex><span>        B_inv <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(L<span style=color:#f92672>.</span>T),np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(L))
</span></span><span style=display:flex><span>        b <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(W,f) <span style=color:#f92672>+</span> d
</span></span><span style=display:flex><span>        a <span style=color:#f92672>=</span> b <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(W), B_inv)<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(W))<span style=color:#f92672>.</span>dot(K)<span style=color:#f92672>.</span>dot(b)
</span></span><span style=display:flex><span>        f <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(K,a)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>#f = np.dot(np.linalg.inv(np.linalg.inv(K)+W), np.dot(W,f) + d)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        <span style=color:#75715e>#if _ %10 == 0:</span>
</span></span><span style=display:flex><span>            <span style=color:#75715e>#print(-0.5*np.inner(a,f) + np.sum(log_lik(y,f)) - np.sum(np.log(np.diag(L))))</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> f
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>f <span style=color:#f92672>=</span> fit_laplace(y_train, K)
</span></span></code></pre></div><p>Train fit</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_train,sigma(f))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x_train, y_train)
</span></span></code></pre></div><p><img src=GP_fit_algo_9_1.png alt=png></p><p>MAP prediction using $\hat{\pi} = \sigma(E(f_{new}))$, where $E(f_{new}) = k_{new}^{T}K^{-1}\hat{f} = k_{new}^{T} \nabla \log p(y |f)$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>d<span style=color:#f92672>=</span> y_train <span style=color:#f92672>-</span> sigma(f)
</span></span><span style=display:flex><span>y_pred <span style=color:#f92672>=</span> sigma(np<span style=color:#f92672>.</span>dot(Kstar<span style=color:#f92672>.</span>T, d))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_test,y_pred)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x_test, y_test)
</span></span></code></pre></div><p><img src=GP_fit_algo_11_1.png alt=png></p><p>We can also predict using averaged predictive probability, $\hat{\pi} = \int \sigma(f_{new}) q(f_{new},X,y,x_{new}) df_{new}$, the integral is not tractable so the logistic is usually approximated by the probit.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>W <span style=color:#f92672>=</span> <span style=color:#f92672>-</span>np<span style=color:#f92672>.</span>diag(<span style=color:#f92672>-</span>sigma(f)<span style=color:#f92672>*</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>sigma(f)))
</span></span><span style=display:flex><span>B <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>identity(W<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(W), K)<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(W))
</span></span><span style=display:flex><span>L <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>cholesky(B)
</span></span><span style=display:flex><span>f_star <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(Kstar<span style=color:#f92672>.</span>T, d)
</span></span><span style=display:flex><span><span style=color:#75715e># need variance as well</span>
</span></span><span style=display:flex><span>middle_part <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sqrt(W)<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(L<span style=color:#f92672>.</span>T))<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(L))<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(W))
</span></span><span style=display:flex><span>var_f <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>diag(Ktest) <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>diag(np<span style=color:#f92672>.</span>dot(Kstar<span style=color:#f92672>.</span>T, middle_part)<span style=color:#f92672>.</span>dot(Kstar))
</span></span><span style=display:flex><span><span style=color:#75715e># probit approximation</span>
</span></span><span style=display:flex><span>lamda_sq <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>pi<span style=color:#f92672>/</span><span style=color:#ae81ff>8.0</span>
</span></span><span style=display:flex><span>kappa <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span> <span style=color:#f92672>/</span> np<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>1.0</span><span style=color:#f92672>/</span>lamda_sq <span style=color:#f92672>+</span>  var_f)
</span></span><span style=display:flex><span>y_pred_laplace <span style=color:#f92672>=</span> sigma(kappa <span style=color:#f92672>*</span> f_star)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_test,y_pred_laplace)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x_test, y_test)
</span></span></code></pre></div><p><img src=GP_fit_algo_13_1.png alt=png></p><p>We can also plot the latent $f$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_test, f_star, color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;green&#39;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>fill_between(x_test<span style=color:#f92672>.</span>ravel(), 
</span></span><span style=display:flex><span>                 f_star <span style=color:#f92672>+</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>sqrt(var_f), 
</span></span><span style=display:flex><span>                 f_star <span style=color:#f92672>-</span> <span style=color:#ae81ff>2</span><span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>sqrt(var_f), 
</span></span><span style=display:flex><span>                 color<span style=color:#f92672>=</span><span style=color:#e6db74>&#39;lightblue&#39;</span>)
</span></span></code></pre></div><p><img src=GP_fit_algo_15_1.png alt=png></p><h1 id=expectation-propagation>Expectation Propagation</h1><p>We want to approximate by approximating the individual likelihoods with a normal</p><p>$$ p(y_i|f_i) \approx t_i(f_i) = N(f_i| \tilde{\mu}_i, \tilde{\sigma}_i) $$</p><p>Call $q(f|\mu, \Sigma)$ the approximation of the posterior $p(f|X,y)$. The EP algorithm does the following:</p><ul><li>Message elimination: Choose a $t_i$ to perform an approximation. Create a cavity distribution by removing $t_i$ from current approximation $q^{-i}= \frac{q}{t_i}$</li><li>Belief projection: Find $q_{new}$ that minimizes $KL(\hat{p}|| q)$, where $\hat{p}(f_i) \propto q^{-i}(f_i) p(y_i|f_i)$. As $q(f_i)$ is normal this is equal to moment matching.</li><li>Message update: Calcuate the new approximatiing factor $t_i \propto \frac{q_{new}}{q^{-i}}$</li></ul><p>Define $\tilde{\tau}_{i}=\tilde{\sigma} _{i}^{-2}$, $\tilde{\nu}= \tilde{S}\tilde{\mu}$, $\tilde{S} = diag(\tilde{\tau})$, $\tilde{\tau} _{-i} = \tilde{\sigma} _{-i}^{-2}$ and $\tilde{\nu} _{-i} = \tilde{\tau} _{-i} \tilde{\mu} _{-i}$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>ep_gp</span>(K, y):
</span></span><span style=display:flex><span>    y_ <span style=color:#f92672>=</span> y<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>    y_[y_ <span style=color:#f92672>==</span> <span style=color:#ae81ff>0</span>] <span style=color:#f92672>=</span> <span style=color:#f92672>-</span><span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#75715e># initialize parameters of q</span>
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>    nu_tilde <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(n)
</span></span><span style=display:flex><span>    tau_tilde <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(n)
</span></span><span style=display:flex><span>    mu <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros(n)
</span></span><span style=display:flex><span>    Sigma <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>copy() <span style=color:#75715e>#+ np.identity(n)</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> i_outer <span style=color:#f92672>in</span> range(<span style=color:#ae81ff>20</span>):
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> range(n):
</span></span><span style=display:flex><span>            <span style=color:#75715e># cavity parameters</span>
</span></span><span style=display:flex><span>            tau_cav <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>reciprocal(Sigma[j,j]) <span style=color:#f92672>-</span> tau_tilde[j]
</span></span><span style=display:flex><span>            nu_cav <span style=color:#f92672>=</span>  np<span style=color:#f92672>.</span>reciprocal(Sigma[j,j])<span style=color:#f92672>*</span>mu[j] <span style=color:#f92672>-</span> nu_tilde[j]
</span></span><span style=display:flex><span>            mu_cav <span style=color:#f92672>=</span> nu_cav<span style=color:#f92672>/</span>tau_cav
</span></span><span style=display:flex><span>            sigma_cav <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span><span style=color:#f92672>/</span>tau_cav
</span></span><span style=display:flex><span>            <span style=color:#75715e># match moment</span>
</span></span><span style=display:flex><span>            z <span style=color:#f92672>=</span> (y_[j]<span style=color:#f92672>*</span>mu_cav)<span style=color:#f92672>/</span>np<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>1</span><span style=color:#f92672>+</span>sigma_cav)
</span></span><span style=display:flex><span>            mu_hat <span style=color:#f92672>=</span> mu_cav  <span style=color:#f92672>+</span> y_[j]<span style=color:#f92672>*</span>sigma_cav<span style=color:#f92672>*</span>norm<span style=color:#f92672>.</span>pdf(z)<span style=color:#f92672>/</span>(norm<span style=color:#f92672>.</span>cdf(z)<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>1</span><span style=color:#f92672>+</span>sigma_cav))
</span></span><span style=display:flex><span>            sigma_hat <span style=color:#f92672>=</span> sigma_cav <span style=color:#f92672>-</span> sigma_cav<span style=color:#f92672>**</span><span style=color:#ae81ff>2</span> <span style=color:#f92672>*</span> norm<span style=color:#f92672>.</span>pdf(z) <span style=color:#f92672>*</span>(z <span style=color:#f92672>+</span> norm<span style=color:#f92672>.</span>pdf(z)<span style=color:#f92672>/</span>norm<span style=color:#f92672>.</span>cdf(z))<span style=color:#f92672>/</span>((<span style=color:#ae81ff>1</span><span style=color:#f92672>+</span>sigma_cav)<span style=color:#f92672>*</span>norm<span style=color:#f92672>.</span>cdf(z))
</span></span><span style=display:flex><span>            <span style=color:#75715e># Message update</span>
</span></span><span style=display:flex><span>            delta_tau <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>reciprocal(sigma_hat) <span style=color:#f92672>-</span> tau_cav <span style=color:#f92672>-</span> tau_tilde[j]
</span></span><span style=display:flex><span>            tau_tilde[j] <span style=color:#f92672>=</span> tau_tilde[j] <span style=color:#f92672>+</span> delta_tau 
</span></span><span style=display:flex><span>            nu_tilde[j] <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>reciprocal(sigma_hat)<span style=color:#f92672>*</span>mu_hat <span style=color:#f92672>-</span> nu_cav
</span></span><span style=display:flex><span>            Sigma <span style=color:#f92672>=</span> Sigma <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>reciprocal((np<span style=color:#f92672>.</span>reciprocal(delta_tau) <span style=color:#f92672>+</span> Sigma[j,j]))<span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>outer(Sigma[:,j],Sigma[:,j])
</span></span><span style=display:flex><span>            mu <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(Sigma,nu_tilde)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>        S <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>diag(tau_tilde)
</span></span><span style=display:flex><span>        L <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>cholesky(np<span style=color:#f92672>.</span>identity(n) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(S),K)<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(S)))
</span></span><span style=display:flex><span>        V <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(L<span style=color:#f92672>.</span>T), np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(S), K))
</span></span><span style=display:flex><span>        Sigma <span style=color:#f92672>=</span> K <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>dot(V<span style=color:#f92672>.</span>T, V)
</span></span><span style=display:flex><span>        mu <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(Sigma, nu_tilde)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> nu_tilde, tau_tilde
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>nu_tilde, tau_tilde <span style=color:#f92672>=</span> ep_gp(K, y_train)
</span></span></code></pre></div><p>With these parameters, we can get the mean of the latent Gaussian process (train) using the same equations as for &ldquo;normal&rdquo; Gaussian processes as we have approximated the likelihood with a normal</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#75715e># get mean of latent gp</span>
</span></span><span style=display:flex><span>mu_tilde <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>diag(np<span style=color:#f92672>.</span>reciprocal(tau_tilde)), nu_tilde)
</span></span><span style=display:flex><span>S <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>diag(tau_tilde)
</span></span><span style=display:flex><span>B <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>identity(K<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(S),K)<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(S))<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(B))<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(S))<span style=color:#f92672>.</span>dot(K)
</span></span><span style=display:flex><span>Sigma <span style=color:#f92672>=</span> K <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>dot(K,np<span style=color:#f92672>.</span>sqrt(S))
</span></span><span style=display:flex><span>f_mean <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(Sigma, np<span style=color:#f92672>.</span>diag(tau_tilde))<span style=color:#f92672>.</span>dot(mu_tilde)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_train, f_mean)
</span></span></code></pre></div><p><img src=GP_fit_algo_21_1.png alt=png></p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_train,sigma(f_mean))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x_train, y_train)
</span></span></code></pre></div><p><img src=GP_fit_algo_22_1.png alt=png></p><p>And we can predict using for example average probabilities</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>predict_ep</span>(nu, tau, K, Kstar, Ktest):
</span></span><span style=display:flex><span>    n <span style=color:#f92672>=</span> K<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    S <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>diag(tau_tilde)
</span></span><span style=display:flex><span>    L <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>cholesky(np<span style=color:#f92672>.</span>identity(n) <span style=color:#f92672>+</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(S),K)<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(S)))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    z <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(S), np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(L<span style=color:#f92672>.</span>T))<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(L))<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>sqrt(S))<span style=color:#f92672>.</span>dot(K)<span style=color:#f92672>.</span>dot(nu)
</span></span><span style=display:flex><span>    fstar <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(Kstar<span style=color:#f92672>.</span>T,nu <span style=color:#f92672>-</span> z)
</span></span><span style=display:flex><span>    v <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(L), np<span style=color:#f92672>.</span>sqrt(S))<span style=color:#f92672>.</span>dot(Kstar)
</span></span><span style=display:flex><span>    V <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>diag(Ktest) <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>diag(np<span style=color:#f92672>.</span>dot(v<span style=color:#f92672>.</span>T,v))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> norm<span style=color:#f92672>.</span>cdf(fstar<span style=color:#f92672>/</span>np<span style=color:#f92672>.</span>sqrt(<span style=color:#ae81ff>1</span><span style=color:#f92672>+</span>V))
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>y_pred_ep <span style=color:#f92672>=</span> predict_ep(nu_tilde, tau_tilde, K, Kstar, Ktest)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_test,y_pred_ep)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x_test, y_test)
</span></span></code></pre></div><p><img src=GP_fit_algo_25_1.png alt=png></p><h1 id=mcmc>MCMC</h1><p>Using MCMC we can sample local regions instead of iteratively drawing samples from each posterior conditional density $p(f_i| f_{i},y)$ separately. If $f_k$ are function points in the region $k$. Then we can propose values from the conditional GP prior $Q(f^{t}|f^{t-1})= p(f_k^{t} | f_{-k}^{t-1})$. The proposed points are accepted with probability $\min(1,A)$ where</p><p>$$
\begin{split}
A &= \frac{p(f^t | y)/Q(f^t|f^{t-1})}{p(f^{t-1} | y)/Q(f^{t-1}|f^{t})} \\
&= \frac{p(y|f_k^t,f_{-k}^{t-1})p(f_k^t,f_{-k}^{t-1})p(f_k^{t-1}|f_{-k}^{t-1})}{p(y|f_k^{t-1},f_{-k}^{t-1})p(f_k^{t-1},f_{-k}^{t-1})p(f_k^{t}|f_{-k}^{t-1})} \\
&= \frac{p(y|f_k^t,f_{-k}^{t-1})p(f_k^t|f_{-k}^{t-1})p(f_{-k}^t)p(f_k^{t-1}|f_{-k}^{t-1})}{p(y|f_k^{t-1},f_{-k}^{t-1})p(f_k^{t-1} | f_{-k}^{t-1})p(f_{-k}^{t-1})p(f_k^{t}|f_{-k}^{t-1})} \\
&= \frac{p(y|f_k^t,f_{-k}^{t-1})}{p(y|f_k^{t-1},f_{-k}^{t-1})}
\end{split}
$$</p><p>I like to think that we are conditioning on $f_{-k}^t = f_{-k}^{t-1}$ and that we are taking $t \gets tn_k$ steps where $n_k$ is the number of regions.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span><span style=color:#f92672>from</span> scipy.stats <span style=color:#f92672>import</span> multivariate_normal
</span></span><span style=display:flex><span><span style=color:#66d9ef>def</span> <span style=color:#a6e22e>bernoulli</span>(y,x):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>return</span> sigma(x)<span style=color:#f92672>**</span>y<span style=color:#f92672>*</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>sigma(x))<span style=color:#f92672>**</span>(<span style=color:#ae81ff>1</span><span style=color:#f92672>-</span>y)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>n_t <span style=color:#f92672>=</span> len(y_train)
</span></span><span style=display:flex><span>B <span style=color:#f92672>=</span> <span style=color:#ae81ff>10000</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>f <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((B<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>,n_t))
</span></span><span style=display:flex><span>r <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>zeros((B,n_t))  <span style=color:#75715e># how many accepted</span>
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>index <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>arange(n_t)
</span></span><span style=display:flex><span><span style=color:#66d9ef>for</span> i <span style=color:#f92672>in</span> tqdm<span style=color:#f92672>.</span>tqdm(range(B)):
</span></span><span style=display:flex><span>    <span style=color:#66d9ef>for</span> j <span style=color:#f92672>in</span> np<span style=color:#f92672>.</span>array_split(range(n_t),<span style=color:#ae81ff>5</span>):
</span></span><span style=display:flex><span>        index_not_j <span style=color:#f92672>=</span> index[<span style=color:#f92672>~</span>np<span style=color:#f92672>.</span>isin(index,j)]
</span></span><span style=display:flex><span>        K_inv <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(K[np<span style=color:#f92672>.</span>ix_(index_not_j,index_not_j)] <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-2</span><span style=color:#f92672>*</span>np<span style=color:#f92672>.</span>identity(len(index_not_j)))  <span style=color:#75715e># bottlneck + regularization for condition</span>
</span></span><span style=display:flex><span>        <span style=color:#75715e># conditional mean </span>
</span></span><span style=display:flex><span>        m <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>dot(K[np<span style=color:#f92672>.</span>ix_(j,index_not_j)],K_inv)<span style=color:#f92672>.</span>dot(f[i, index_not_j])
</span></span><span style=display:flex><span>        <span style=color:#75715e># conditional variance</span>
</span></span><span style=display:flex><span>        s <span style=color:#f92672>=</span> K[np<span style=color:#f92672>.</span>ix_(j,j)] <span style=color:#f92672>-</span> np<span style=color:#f92672>.</span>dot(K[np<span style=color:#f92672>.</span>ix_(j,index_not_j)],K_inv)<span style=color:#f92672>.</span>dot(K[np<span style=color:#f92672>.</span>ix_(j,index_not_j)]<span style=color:#f92672>.</span>T)
</span></span><span style=display:flex><span>        f_new <span style=color:#f92672>=</span> f[i,:]<span style=color:#f92672>.</span>copy()
</span></span><span style=display:flex><span>        f_new[j] <span style=color:#f92672>=</span> multivariate_normal<span style=color:#f92672>.</span>rvs(mean <span style=color:#f92672>=</span> m, cov <span style=color:#f92672>=</span> s)
</span></span><span style=display:flex><span>        y_lik_new <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum([np<span style=color:#f92672>.</span>log(bernoulli(y_train[k],f_new[k])) <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(n_t)])
</span></span><span style=display:flex><span>        y_lik_old <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>sum([np<span style=color:#f92672>.</span>log(bernoulli(y_train[k],f[i,k])) <span style=color:#66d9ef>for</span> k <span style=color:#f92672>in</span> range(n_t)])
</span></span><span style=display:flex><span>        
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>if</span> np<span style=color:#f92672>.</span>log(np<span style=color:#f92672>.</span>random<span style=color:#f92672>.</span>uniform()) <span style=color:#f92672>&lt;=</span> np<span style=color:#f92672>.</span>min((y_lik_new <span style=color:#f92672>-</span> y_lik_old,<span style=color:#ae81ff>0</span>)):
</span></span><span style=display:flex><span>            f[i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>,j] <span style=color:#f92672>=</span> f_new[j]
</span></span><span style=display:flex><span>            r[i,j] <span style=color:#f92672>=</span> <span style=color:#ae81ff>1</span>
</span></span><span style=display:flex><span>        <span style=color:#66d9ef>else</span>:
</span></span><span style=display:flex><span>            f[i<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>,j] <span style=color:#f92672>=</span> f[i,j]
</span></span></code></pre></div><pre><code>100%|██████████| 10000/10000 [00:22&lt;00:00, 449.70it/s]
</code></pre><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>np<span style=color:#f92672>.</span>mean(r,<span style=color:#ae81ff>0</span>)
</span></span></code></pre></div><pre><code>array([0.7516, 0.7516, 0.7516, 0.7516, 0.8562, 0.8562, 0.8562, 0.8562,
       0.9313, 0.9313, 0.9313, 0.9313, 0.8495, 0.8495, 0.8495, 0.8495,
       0.505 , 0.505 , 0.505 , 0.505 ])
</code></pre><p>We can also plot traceplot of one $f_i$</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(range(B<span style=color:#f92672>+</span><span style=color:#ae81ff>1</span>), f[:,<span style=color:#ae81ff>3</span>])
</span></span></code></pre></div><p><img src=GP_fit_algo_40_1.png alt=png></p><p>Very autocorrelated, might have to do thinning, test other regions, increase regularization etc..</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>burning <span style=color:#f92672>=</span> <span style=color:#ae81ff>2000</span>
</span></span><span style=display:flex><span>f_latent_m <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>mean(f[burning:,],<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>f_latent_s <span style=color:#f92672>=</span> np<span style=color:#f92672>.</span>std(f[burning:,],<span style=color:#ae81ff>0</span>)
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span>ci <span style=color:#f92672>=</span> <span style=color:#ae81ff>1.96</span><span style=color:#f92672>*</span>f_latent_s
</span></span><span style=display:flex><span>fig, ax <span style=color:#f92672>=</span> plt<span style=color:#f92672>.</span>subplots(<span style=color:#ae81ff>1</span>,<span style=color:#ae81ff>1</span>)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>plot(x_train, f_latent_m)
</span></span><span style=display:flex><span>ax<span style=color:#f92672>.</span>fill_between(x_train, (f_latent_m<span style=color:#f92672>-</span>ci), (f_latent_m <span style=color:#f92672>+</span> ci), color <span style=color:#f92672>=</span> <span style=color:#e6db74>&#39;b&#39;</span>, alpha <span style=color:#f92672>=</span> <span style=color:#ae81ff>.1</span>)
</span></span></code></pre></div><p><img src=GP_fit_algo_42_1.png alt=png></p><p>Instead of doing simulations $f(f_{new}|X,y,x_{new}) = \int p(f_{new}|X,x_{new},f)p(f|X,y)df $ we can approximatione the $f_{new}$ as $N(E[f], V[f])$.</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>f_m_mcmc <span style=color:#f92672>=</span>   np<span style=color:#f92672>.</span>dot(Kstar<span style=color:#f92672>.</span>T,np<span style=color:#f92672>.</span>linalg<span style=color:#f92672>.</span>inv(K <span style=color:#f92672>+</span> <span style=color:#ae81ff>1e-3</span>{new}np<span style=color:#f92672>.</span>identity(K<span style=color:#f92672>.</span>shape[<span style=color:#ae81ff>0</span>]))<span style=color:#f92672>.</span>dot(f_latent_m))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_test, sigma(f_m_mcmc))
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x_test, y_test)
</span></span></code></pre></div><p><img src=GP_fit_algo_44_1.png alt=png></p><h1 id=compare>Compare</h1><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-python data-lang=python><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_test,y_pred_ep, label <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;EP&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_test,y_pred_laplace, label <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;Laplace&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>plot(x_test,sigma(f_m_mcmc), label <span style=color:#f92672>=</span> <span style=color:#e6db74>&#34;MCMC&#34;</span>)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>scatter(x_test, y_test)
</span></span><span style=display:flex><span>plt<span style=color:#f92672>.</span>legend()
</span></span></code></pre></div><p><img src=GP_fit_algo_46_1.png alt=png></p><p>EP and VI give a little strange results.</p></div></div></div><div class="layui-col-md4 layui-col-sm12 layui-col-xs12"><div class="layui-card single-card"><h2 class=single-title>Relevant Topics</h2><div style=margin-left:10px><blockquote class="self-elem-quote self-elem-quote-bg-red" style=background-color:#fff;margin-top:10px><a href=/post/deepkernel/><h3>Deep Graph Kernels</h3></a><h3 style=margin-top:10px;margin-bottom:10px><i class="layui-icon layui-icon-date" style=font-size:28px;vertical-align:-2px></i>
<span>2021-11-16</span>
<i class="layui-icon layui-icon-tabs" style=font-size:22px;vertical-align:1px;margin-right:2px></i>
<a href=/tags/graph/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Graph</span></a>
<a href=/tags/classification/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Classification</span></a>
<a href=/tags/kernel/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Kernel</span></a></h3></blockquote></div><div style=margin-left:10px><blockquote class="self-elem-quote self-elem-quote-bg-red" style=background-color:#fff;margin-top:10px><a href=/post/depgp/><h3>Dependent Gaussian Processes</h3></a><h3 style=margin-top:10px;margin-bottom:10px><i class="layui-icon layui-icon-date" style=font-size:28px;vertical-align:-2px></i>
<span>2022-06-30</span>
<i class="layui-icon layui-icon-tabs" style=font-size:22px;vertical-align:1px;margin-right:2px></i>
<a href=/tags/kernel/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Kernel</span></a></h3></blockquote></div><div style=margin-left:10px><blockquote class="self-elem-quote self-elem-quote-bg-red" style=background-color:#fff;margin-top:10px><a href=/post/hmm/><h3>Hidden Markov Models</h3></a><h3 style=margin-top:10px;margin-bottom:10px><i class="layui-icon layui-icon-date" style=font-size:28px;vertical-align:-2px></i>
<span>2022-02-15</span>
<i class="layui-icon layui-icon-tabs" style=font-size:22px;vertical-align:1px;margin-right:2px></i>
<a href=/tags/model/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Model</span></a>
<a href=/tags/classification/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Classification</span></a></h3></blockquote></div><br></div><div class="layui-card single-card"><h2 class=single-title>Recent Posts</h2><div style=margin-left:10px><blockquote class="self-elem-quote self-elem-quote-bg-red" style=background-color:#fff;margin-top:10px><a href=/post/gp_fit_algo/><h3>Gaussian Process Classification</h3></a><h3 style=margin-top:10px;margin-bottom:10px><i class="layui-icon layui-icon-date" style=font-size:28px;vertical-align:-2px></i>
<span>2022-09-18</span>
<i class="layui-icon layui-icon-tabs" style=font-size:22px;vertical-align:1px;margin-right:2px></i>
<a href=/tags/classification/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Classification</span></a>
<a href=/tags/kernel/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Kernel</span></a></h3></blockquote></div><div style=margin-left:10px><blockquote class="self-elem-quote self-elem-quote-bg-red" style=background-color:#fff;margin-top:10px><a href=/post/depgp/><h3>Dependent Gaussian Processes</h3></a><h3 style=margin-top:10px;margin-bottom:10px><i class="layui-icon layui-icon-date" style=font-size:28px;vertical-align:-2px></i>
<span>2022-06-30</span>
<i class="layui-icon layui-icon-tabs" style=font-size:22px;vertical-align:1px;margin-right:2px></i>
<a href=/tags/kernel/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Kernel</span></a></h3></blockquote></div><div style=margin-left:10px><blockquote class="self-elem-quote self-elem-quote-bg-red" style=background-color:#fff;margin-top:10px><a href=/post/nnfun/><h3>Neural Networks for Insurance Pricing</h3></a><h3 style=margin-top:10px;margin-bottom:10px><i class="layui-icon layui-icon-date" style=font-size:28px;vertical-align:-2px></i>
<span>2022-04-11</span>
<i class="layui-icon layui-icon-tabs" style=font-size:22px;vertical-align:1px;margin-right:2px></i>
<a href=/tags/model/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Model</span></a>
<a href=/tags/neural-network/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Neural Network</span></a>
<a href=/tags/insurance/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Insurance</span></a></h3></blockquote></div><div style=margin-left:10px><blockquote class="self-elem-quote self-elem-quote-bg-red" style=background-color:#fff;margin-top:10px><a href=/post/glm/><h3>Generalized Linear Models</h3></a><h3 style=margin-top:10px;margin-bottom:10px><i class="layui-icon layui-icon-date" style=font-size:28px;vertical-align:-2px></i>
<span>2022-03-02</span>
<i class="layui-icon layui-icon-tabs" style=font-size:22px;vertical-align:1px;margin-right:2px></i>
<a href=/tags/model/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Model</span></a></h3></blockquote></div><div style=margin-left:10px><blockquote class="self-elem-quote self-elem-quote-bg-red" style=background-color:#fff;margin-top:10px><a href=/post/hmm/><h3>Hidden Markov Models</h3></a><h3 style=margin-top:10px;margin-bottom:10px><i class="layui-icon layui-icon-date" style=font-size:28px;vertical-align:-2px></i>
<span>2022-02-15</span>
<i class="layui-icon layui-icon-tabs" style=font-size:22px;vertical-align:1px;margin-right:2px></i>
<a href=/tags/model/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Model</span></a>
<a href=/tags/classification/><span class="layui-badge layui-bg-orange" style=vertical-align:2px>Classification</span></a></h3></blockquote></div><br></div></div></div></div></div><footer><div class=layui-container><p class=copyright>&copy; Powered by <a href=https://gohugo.io style=color:#fff>Hugo</a>, <a href=https://github.com/ertuil/erblog style=color:#fff>Erblog</a> and <a href=https://github.com/vlunot/nb2hugo style=color:#fff>nb2hugo</a>.</p></div></footer></body></html>