<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Posts on Ragnar</title><link>https://ragnarlevi.github.io/post/</link><description>Recent content in Posts on Ragnar</description><generator>Hugo -- gohugo.io</generator><language>en</language><lastBuildDate>Sun, 18 Sep 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://ragnarlevi.github.io/post/index.xml" rel="self" type="application/rss+xml"/><item><title>Gaussian Process Classification</title><link>https://ragnarlevi.github.io/post/gp_fit_algo/</link><pubDate>Sun, 18 Sep 2022 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/gp_fit_algo/</guid><description>In this notebook I will be trying to fit a Gaussian Process classification from scratch on very simple simulated data. I will use the laplacian, expectation propagation, variational inference (which will need some rework) and MH MCMC.
import numpy as np import matplotlib.pyplot as plt from sklearn.metrics.pairwise import rbf_kernel import seaborn as sns from scipy.stats import norm from scipy.optimize import minimize import tqdm Create dataset
x = np.array(list(range(100))) y = np.</description></item><item><title>Dependent Gaussian Processes</title><link>https://ragnarlevi.github.io/post/depgp/</link><pubDate>Thu, 30 Jun 2022 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/depgp/</guid><description>Dependent Gaussian Processes In this workbook, I am going to reproduce the work of Phillip Boyle and Marcus Frean Dependent Gaussian Processes. I will assume that the reader is familiar with the basics of Gaussian Processes
import numpy as np import matplotlib.pyplot as plt import seaborn as sns Consider a device that operates on a continuous, real valuedinput signal overtime $x(t)$ and emits a continuous real valued output $y(t)$. This device is a linear time invariant (LTI) fiter if it is</description></item><item><title>Neural Networks for Insurance Pricing</title><link>https://ragnarlevi.github.io/post/nnfun/</link><pubDate>Mon, 11 Apr 2022 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/nnfun/</guid><description>In this notebook we will be looking at a typicall insurance pricing data set and test GLM and NN models. A typical assumption is that the repsone is poisson: $$ P(Y = y) = \frac{\lambda^y \exp(-\lambda)}{y!}$$ To note a shortcoming of the Poisson distribution is that the mean is equal to the variance, and thus one might use quasi-Poisson or a negative binomial instead. Another shortcoming is that insurance claims are usally zero inflated and thus a zero-inflated poisson model or a hurdle poisson model might be used instead.</description></item><item><title>Generalized Linear Models</title><link>https://ragnarlevi.github.io/post/glm/</link><pubDate>Wed, 02 Mar 2022 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/glm/</guid><description>In this notebook, I am going to note down elements of generalized linear models (GLM). GLMs come from the exponential family where a response $y$ has the following distribution $$f(y;\theta) =\exp\Big(\frac{y\theta - b(\theta)}{a(\phi)} + c(y,\theta)\Big)$$ where $\theta$ is the canonical parameter and $\phi$ is the dispersion parameter. Before continuing we define the score and derive the a formula for the expected score and the information.
The score is the derivative of the log-likelihood and we can derive the expected score as, using $f = f(y;\theta)$ :</description></item><item><title>Hidden Markov Models</title><link>https://ragnarlevi.github.io/post/hmm/</link><pubDate>Tue, 15 Feb 2022 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/hmm/</guid><description>In this notebook I will be exploring a Hidden Markov Model (HMM) to classify bull and bear states of a simulated financial instrument.
library(depmixS4) library(ggplot2) library(matrixStats) Simulate the data. Assume returns are normally distributed. Where the mean and the standard deviation depends on the state of the financial market. We will only consider 2 states.
set.seed(42) Nk_lower &amp;lt;- 50 Nk_upper &amp;lt;- 150 bull_mean &amp;lt;- 0.1 bull_var &amp;lt;- 0.1 bear_mean &amp;lt;- -0.</description></item><item><title>Quntitative Risk Analysis</title><link>https://ragnarlevi.github.io/post/extremes/</link><pubDate>Wed, 17 Nov 2021 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/extremes/</guid><description>In this notebook I will discuss the basic ideas of modelling quantiles and copula dependence. These ideas are important for risk managers as International banking regulations require banks to pay specific attention to the probability of large losses over short periods of time and undeerestimation of dependence among extreme risks can lead to serious consequences, as for instance those we experienced during the last financial crisis. Including extreme risks in probabilistic models is recognized nowadays as a necessary condition for good risk management in any institution, and not restricted anymore to reinsurance companies, who are the providers of covers for natural catastrophes.</description></item><item><title>Deep Graph Kernels</title><link>https://ragnarlevi.github.io/post/deepkernel/</link><pubDate>Tue, 16 Nov 2021 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/deepkernel/</guid><description>Deep Graph Kernels In this notebook I will be constructing a deep graph kernel based on this paper. It utilizes the Weisfeiler-Lehman isomorphism test algorithm and/or the shortest-path algorithm. The kernel is then used for graph classification. The code is taken from jcatw which again cites Pinar Yanardag as the original author. The code is adjusted to accommodate the networkx library and python 3.
import networkx as nx import pandas as pd import numpy as np import re from nltk.</description></item><item><title>Group Identification in Stock Markets</title><link>https://ragnarlevi.github.io/post/correlationnetwork/</link><pubDate>Thu, 16 Sep 2021 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/correlationnetwork/</guid><description>Introduction In this notebook I will be trying to replicate the work of Dong-Hee Kim and Hawoong Jeong. The goal is to look at the eigen decomposition of the return correlation matrix. The decomposition allows us to separate the correlation into three parts. Namely, the marketwide effect, group correlation matrix and a random noise part. By filtering out the marketwide effect and the random noise, we can look at nontrivial correlation of stock groups.</description></item><item><title>Data Visualization and Data Reduction</title><link>https://ragnarlevi.github.io/post/data_visualization/</link><pubDate>Mon, 30 Aug 2021 00:00:00 +0000</pubDate><guid>https://ragnarlevi.github.io/post/data_visualization/</guid><description>Introduction In this notebook I will be trying out some dimensionality reduction techniques which can be used for modelling and/or data visualization. Dimensional reduction occupies a central position in many fields. In essence, the goal is to change the representation of data sets, originally in a form involving a large number of variables, into a low-dimensional description. The main difference between data reduction and data visualization is that some data visualization techniques can not be used on unseen data, thus they will not be useful in the modelling part.</description></item></channel></rss>